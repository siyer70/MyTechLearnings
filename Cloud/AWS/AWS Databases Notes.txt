Database
=======
RDS (OLTP - Online Transaction Processing)
	- Supported Relational Databases
		- Aurora
		- SQL Server
		- Oracle
		- MYSQL Server
		- Postgres
		- MariaDB
	- Two key features of RDS
		- Multi-AZ - for Disaster Recovery
			- Exact copy of your database in another AZ
			- AWS handles the replication for you - all writes are synchronized
			- Uses Primary Instance and Secondary Instances concept
			- Connection String refers to DNS name that is hosted by AWS
			- AWS internally points the DNS to Primary Instance IP
			- If primary is not available, it updates the DNS to point to one of the secondary instance
			- so there is an automatic fail over to the secondary
			- ex scenarios - Planned DB Maintenance, DB Instance failure, AZ failure
			- all supported RDS DBs have Multi-AZ option except Aurora which manages this in a different manner
		- Read Replicas - For Performance
			- used for scaling, not for DR
			- Must have automated backups (described below) turned on in order to deploy a read replica
			- created for handling read only requests to improve performance
			- asynchronously replication from the primary RDS instance to read replicas
			- No automatic failover - if primary fails, you are not automatically routed to the read replica
			- You need to explicitly point to the read replica URL to read from it (only read)
			- You can have upto 5 read replicas
			- you can create read replicas of read replicas (but watch out for latency)
			- you can have read replicas that have Multi-AZ
			- you can create read replicas of Multi-AZ source databases
			- you can also have read replica in second region
			- read replicas can be promoted to be their own databases - this breaks the replication
			- Use case
				- if you have too many read queries and very few updates, it makes sense to setup read replicas
				- Blog sites are the best example
			- only following DBs support read replicas - MYSQL, Postgres, MariaDB, Aurora
			- following DBs does not support read replicas - Oralce, SQL Server
		- RDS runs on virtual machines
		- you cannot log into these os
		- patching of OS and DB is AWS responsibility
		- RDS is not serverless
		- but Aurora Serverless is serverless
		- you can switch your DNS to point to the secondary instance explicitly by “Reboot with failover” option
			- first select “Reboot” and it will prompt for “Reboot with failover”
		- you can migrate the supported RDS instance to a “Aurora Instance” by using the option
			- “Create Aurora Read Replica” 
			- Once the Aurora instance is created, you can then promote the instance to be master/primary db
	- Two different types of backups for RDS
		- Automated Backups
			- recover your database to any point in time within a “Retention Period”
			- Retention Period can be between 1 and 35
			- Automated backups will take a full daily snapshot
			- It will also store transaction logs throughout the day
			- During recovery
				- It will take the most recent daily backup and then apply transaction logs relevant to that day
				- This will allow you to do a point in time recovery down to a second within the retention period
			- Automated backups are enabled by default
				- backup data is stored in s3 and you get free storage space equal to the size of the db
			- backups are taken during a defined window 
			- storage I/O may be suspended while your data is being backup up
			- you may experience elevated latency because of the above
		- Database Snapshots
			- user initiated (done manually)
			- they are stored even after you delete your original RDS instance, unlike automated backups
			- while deleting your RDS instance, you are prompted to take the final snapshot of the db
		- When you restore DB (either Automatic or Snapshot) 
			- the restored version will be a new RDS instance with a new DNS endpoint
	- Encryption
		- All supported DBs provide encryption at rest
		- Encryption done using Key Management Service (KMS)
		- Once the RDS is encrypted
			- Data stored at rest in the underlying storage is encrypted
			- Same applies to automated backups, read replicas and snapshots

	- Database Instances and Number of database limitations
		- 40 DB instances can be launched - (10 Oracle or MS SQL Server) or all 40 can be Arora or postgres or mysql or maria or even Oracle if BYOL
		- Oracle only one database per instance where as MS SQL Server allows upto 100 databases
		- No limitations on Arora, Postgres, MYSQL, Maria

	- Billing is based on
		- DB Instance hours
		- Storage (per GB per month)
		- Provisioned IOPS per month
		- I/O requests per month
		- Backup storage
		- Data transfer (In and Out)
		

Datawarehouse (OLAP - Online Analytical Processing)
	- Used for business Intelligence 
	- Tools like Cognos, Jaspersoft, SQL Server Reporting Services, Oracle, Hyperion, SAP NetWeaver
	- Used to pull in very large and complex data sets for querying purpose
	- Uses a different type of architecture both from a database perspective and infrastructure layer
	- Amazon’s Datawarehouse solution is called “Redshift”

DynamoDB (NoSQL)
	- fast and flexible NoSQL database service 
	- serverless
	- for all applications that need consistent, single-digit millisecond latency at any scale
	- fully managed database
	- supports both key-value and document models
	- flexible data model and reliable performance
	- great fit for mobile, web, gaming, ad-tech, IOT etc
	- stored on SSD storage
	- spread across 3 geographically distinct data centres
	- eventual consistent reads (default) and strongly consistent reads

	- Throughput basics
		- Determined on the following basis
			- WCU - Write Capacity Units
				- One write per second for an item upto 1kb in size
				- if the items are larger than 1kb, then more WCUs are consumed

				Example 1: we write 10 objects per second of 2kb each = 10 * 2 = 20 WCUs
				Example 2: we write 6 objects per second of 4.5kb each = 6 * 5 = 30 WCUs (4.5kb rounded off to 5)
				Example 3: we write 120 objects per minute of 2 kb each = (120/60) * 2 = 4 WCUs
			- RCU - Read Capacity Units
				- One read per second for strongly consistent reads for an item upto 4 kb in size
				- Two read per second for eventually consistent reads for an item upto 4 kb in size
				- if the items are larger than 4kb, then more RCUs are consumed

				Example 1: 10 strongly consistent reads per second of 4kb each = 10 * (4kb / 4kb) = 10 RCUs
				Example 2: 16 eventually consistent reads per second of 12kb each = (16 / 2) * (12kb/4kb) = 24 RCUs
				Example 3: 10 strong consistently reads per second of 6kb each = 10 * (8/4) = 20 RCUs (6kb rounded off to 8kb - should be multiple of 4kb)

			- Determining the number of partitions:
				- Determine capacity: capacity = (Total RCU / 3000) + (Total WCU / 1000)
				- Determine size: size = size of db / 10 GB
				- Number of partitions: partitions = CEILING(MAX(capacity, size))

				- WCUs and RCUs are evenly spread between partitions (Important)
					- For example: if you have 10 partitions and if you have 100 RCUs and 100 WCUs, then each partition gets 10 each
		- Option to enable Auto-scaling of throughput to meet demand
		- "Burst Credit" from AWS if you exceed little bit more than the provisioned throughput
		- ProvisionedThroughputExceededException if it exceeds provisioned throughput
			- Reasons
				- Hot keys: One partition key is read too many times (for ex: popular item or product)
				- Hot partitions
				- Very large items: Remember WCU and RCU both dependent on the size of items

			- Solutions:
				- Exponential Back off (already in SDK)
				- Distributed partition keys as much as possible
				- If RCU is the issue, then use DAX (DynamoDB Accelerator)

		- It has a feature of "Conditional Update / Delete" that makes DynamoDB an optimistic / concurrency database
			- That means that you can ensure an item hasn't changed before altering it

		- DAX or DynamoDB Accelerator
			- Seamless cache for DynamoDB
			- Writes go through DAX to DynamoDB
			- Micro second latency for cached reads & queries
			- Solves the Hot key problem (too many reads for a partition key ex: popular product)
			- 5 minutes TTL for cache (items) by default

			- To use DAX, create a DAX cluster (cache cluster), specify the node type, size etc..
			- supports up to 10 nodes for the DAX (cache) cluster
			- Multi AZ (3 nodes minimum recommended for production)
			- Secure (Encryption at rest with KMS, VPC, IAM, Cloud Trail)

		- Programming Interfaces
			- High Level Interface
			- Document Interface
			- Low Level Interface

			- Low Level API

		- upto 400 kb of data per item (combined size of attribute Name and value) - Important

			

		- SDK
			- PutItem
				- create or full replace) 
				- consumes WCU
			- UpdateItem 
				- partial update of attributes 
				- possibility to use atomic counters and increase them
			- Conditional Writes 
				- write/update only if a condition matches or reject the write request 
				- helps with concurrent access to items
				- no performance impact

			- DeleteItem
				- Delete an individual row
				- Ability to perform a conditional delete

			- DeleteTable
				- Deletes the entire table and all its items
				- Much quicker deletion than calling DeleteItem on all items

			- BatchWriteItem
				- upto 25 PutItem or DeleteItem in a single call
				- upto 16 MB of data written
				- for partial failures, you have to build the retry mechanism for failed items
					- using exponential back off algorithm
				
				- saves you in latency by reducing the number of API calls done against DynamoDB
				- operations are done in parallel for better efficiency

			- GetItem
				- Read based on partition key
				- hash key or hash-range i.e. partition key or partition key + sort key
				- ProjectionExpression can be specified to include only certain attributes (saves network bandwidth)
				- eventually consistent by default
				- option to use strongly consistent reads (more RCU and might take longer)

			- BatchGetItem
				- upto 100 items
				- upto 16 MB of data
				- items are retrieved in parallel to minimise latency

			- Query
				- returns items based on
					- partition key (must be an = operator)
					- sort key (= < <= > >= between begin) - optional
					- filter expression to further filer (client side filtering)
				- returns 
					- upto 1 mb of data
					- or number of items specified in limit
				- able to do paginations in result
				- can query table, a local secondary index or a global secondary index

			- Scan
				- Scans the entire table and then filter out data (inefficient)
				- Returns upto 1 MB of data - use pagination
				- consumes lot of RCU
				- use with limit option if possible or reduce the size of result and pause
				- use parallel scan for better performance - but RCU consumption will still be very high
				- consider using ProjectionExpression + FilterExpression (no change in RCU)

			- CLI
				--projection-expression - attributes to retrieve
					- example:
						aws dynamodb scan --table-name <table name> --projection-expression <expression> --region <region name>
						aws dynamodb scan --table-name userposts --projection-expression "user_id, content" --region "us-east-1"

				--filter-expression - filter results
					- example:
						aws dynamodb scan --table-name userposts --filter-expresion "user_id = :u" 
							--expression-attribute-values '{":u" : {"S" : "123456"}}' --region "us-east-1"

				- Optimization
					--page-size: full dataset is still retrieved but each API call will request less data (helps avoid time outs)

					aws dynamodb scan --table-name userposts --region "us-east-1" --page-size 1
					
					- if there are 3 rows in DB, it will invoke 3 API calls (since the page size is 1) to get all data but that is transparent to us
					- you would still see 3 records in output
					

				- Pagination
					--max-items: maximum number of results returned by the CLI, Returns NextToken
					--starting-token: specify the last received NextToken to keep on reading

					aws dynamodb scan --table-name userposts --region "us-east-1" --max-items 1

					- here you will see only one record, but with "NextToken" as one of the attribute in the result, next time you can use that
					- if you don't see any "NextToken" in the result, that means that there are no more pages to show

					aws dynamodb scan --table-name userposts --region "us-east-1" --max-items 1 --starting-token "..."

	- LSI (Local secondary index)
		- An index that has the same partition key as the base table, but a different sort key. 
		- Alternate range key for your table, local to the hash key
		- Up to five local secondary indexes per table
		- The sort key consists of exactly one scalar attribute (String, Number, Binary)
		- LSI must be defined at table creation time
		- Uses the WCU and RCU of the main table (no special throttling considerations)

		example player_games table
			player_id (partition key)
			game_id (sort key)
			game_played_on (this can be a candidate LSI)
			

	- GSI
		- An index with a partition key and a sort key that can be different from those on the base table.
		- To speed up queries on non-key attributes, use a Global Secondary Index
		- GSI = partition key + optional sort key
		- The index is a new "table" and we can project attributes on it
			- partition key and sort key of the original table are always projected (key only)
			- can specify extra attributes to project (INCLUDE)
			- can use all attributes from main table (ALL)
		- Must define RCU / WCU for the index (since it is like a table only)
			- If the writes are throttled on the GSI, the main table will be throttled (Important)
			- Assign your WCU capacity carefully and choose your GSI partition key carefully
		- Possibility to add / modify GSI as you go (not like LSI) - One of the key difference between LSI and GSI



		example player_games table
			player_id (partition key of the original table) 
				- one of the attribute to be projected in GSI
			game_id (sort key of the original table) - 
				- this can be a partition / sort key
				- or can be one of the attribute to be projected in GSI
			game_played_on (this can be a candidate LSI) 
				- this can be a partition / sort key
				- or one of the attribute to be projected in GSI

	- DynamoDB Streams
		- Changes in DynamoDB (Create, Update, Delete) can end up in a DynamoDB stream
		- Flow
			- Change Log (create / update / delete in table) -> Stream -> Lambda
		- This stream can be read by AWS Lambda and we can then define actions such as:
			- React to changes in real time (for eg: welcome email to users)
			- Analytics
			- Create derivative tables / views
			- Insert into ElasticSearch 
			- and so on & so forth
		- Could implement cross region replication using Streams
		- Streams has 24 hours of data retention

	- DynamoDB Transactions
		- Ability to create / update / delete multiple rows in multiple table at the same time
		- All or nothing type of operation
		- Write Modes now: Standard, Transactional (transactional now added)
		- Same way Read Modes now: Eventual Consistency, Strong Consistency, Transactional
		- Consume 2X of WCU / RCU
	
		- Example
			- AccountBalance Table and Transactions Table

			- AccounntBalance (Account Id, Balance, Last Transaction timestamp)
			- Transactions table (tx id, tx date, account_from, account_to, amt, dc, tag)
		
			- When a transaction occurs, transaction record needs to be inserted in Transactions table and AccountBalance also needs to be updated
		
		
	- DynamoDB TTL (Time to Live for Items/Rows in DynamoDB)
		- You define a TTL column (name it as you want) and add a date as a number there (use epochconverter - time since 1970 - to get java long type value for date)
		- got overview tab in the table and click manage TTL - specify the ttl column name there (you can also run preview from there to see records for specific time)
		- TTL is provided at no extra cost, deletions do not use WCU / RCU
		- TTL is a background task operated by DynamoDB service itself
		- It is enabled per Row
		- It typically deletes expired items within 48 hours of expiration
		- Deleted items due to TTL are also deleted in GSI / LSI
		- DynamoDB Streams can help recover expired items
		Use case
		- Helps reduce storage and manage table size over time
		- Helps adhere to regulatory norms

	- DynamoDB Security
		- VPC Endpoints available for accessing DynamoDB without internet
		- Access fully controlled by IAM
		- Encryption at Rest using KMS
		- Encryption in transit using SSL / TLS
	
	- DynamoDB Backup & Restore
		- Point in time restore like RDS
		- No performance impact (it uses streams internally)

	- DynamoDB Global Table
		- Multi Region, fully replicated, high performance
	
	- Data Migration to DynamoDB
		- You can use Amazon DMS can be used to migrate to DynamoDB (from MongoDB, Oracle, MYSQL, S3 etc)

	- DynamoDB local testing
		- you can launch a local DynamoDB on your computer for development purposes


aws dynamodb put-item \
    --table-name Music \
    --item '{ "Artist": {"S": "Acme Band"}, "SongTitle": {"S": "Happy Day"}, "AlbumTitle": {"S": "Songs About Life"} }' \
    --return-consumed-capacity TOTAL --endpoint-url http://localhost:8000

aws dynamodb query --table-name Music --key-conditions file:///Users/chasrini/aws/key-conditions.json --endpoint-url http://localhost:8000
		
RedShift (Datawarehouse / OLAP)
	- Amazon Datawarehouse solution is called RedShift
	- It is used for business intelligence
	- Fast and powerful, fully managed, petabyte-scale data warehouse service in the cloud
	- Customers can start small for just $0.25 per hour with no commitments or upfront costs
	- scale to petabyte or more for $1000 per terabyte per year
		- less than a tenth of most other datawarehouses solutions
	- OLAP transaction example (pulls large number of records for analytical purpose)
		- Net profit of EMEA and Pacific for the Digital Radio product case study
			- sum of radios sold in each region respectively
			- unit cost for radio in each region respectively
			- sale price of each radio in each region
			- sale price - unit cost of each radio in each (profit for each radio)
			- aggregate net profit for each region
	- Can put severe stress on the database - hence transactional database is always separated
		- to have a separate Datawarehouse for such analytical activities

	- Datawarehouse databases use different type of architecture both from
		- database perspective
		- infrastructure layer
	 
	- Can be configured as follows
		- Single Node (160GB)

		- Multi Node (see following components)
			- Leader Node: Manages client connections and receives queries
			- Compute Node: store data and perform queries and computations (upto 128 compute nodes per leader)

	- Compression
		- Concept
			- Columnar datastore can be compressed much more than row-based data stores
				- because similar data is stored sequentially in database

		- RedShift employees multiple compression techniques and can achieve significant compression
			- relative to traditional relational data stores
			- when loading data into empty table, Redshift automatically samples your data
				- and selects the most appropriate compression scheme
		- Redshift doesn’t require indexes or materialised views and so uses less space than traditional ones
	

	- Massive Parallel Processing (MPP)
		- Automatically distributes data and query load across all nodes
		- Makes it easy to add nodes to your data warehouse and thus enables
			- fast query performance as your data warehouse grows

	- Backups
		- Enabled by default with a 1 day retention period
		- Maximum retention period is 35 days
		- Redshift always attempts to maintain 3 copies of your data
			- Original and replica on the compute nodes and a backup in Amazon S3
		- RedShift can also asynchronously replicate your snapshots to S3 in another region for DR

	- Pricing
		- Based on compute node hours 
			- total number of hours you run across all your compute nodes for the billing period
			- you are billed for 1 unit per node per hour 
				- so a 3-node cluster running persistently for an entire month would incur 2160 instance hours
		- You’ll be charged for backup
		- You’ll be charged for Data transfer (only within a VPC, not outside it)
		- Leader nodes are not charged (only compute nodes are charged)

	- Security Considerations
		- Encrypted in transit using SSL
		- Encrypted at rest using AES-256 encryption
		- By default RedShift takes care of key management
			- Manage your own keys through HSM
			- AWS Key Management Service

	- Availability
		- Currently only available in 1 AZ
		- Can restore snapshots to new AZs in the event of an outage	

ElasticCache
	- A web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud
	- The service improves the performance of web applications and database
		- by allowing you to retrieve information from fast, managed in-memory caches rather than from DB disk files
	- Supports two open source in-memory caching engines
		- Memcached
			- Scale Horizontally - Yes
			- Mullti-Threaded Performance - Yes
			- Advance Data Types - No
			- Ranking/Sorting data sets - No
			- Pub/Sub capabilities - No
			- Persistence - No
			- Multi-AZ - No
			- Backup & Retore Capabilities - No
		- Redis
			- Scale Horizontally - No
			- Mullti-Threaded Performance - No
			- Advance Data Types - Yes
			- Ranking/Sorting data sets - Yes
			- Pub/Sub capabilities - Yes
			- Persistence - Yes
			- Multi-AZ - Yes
			- Backup & Restore Capabilities - Yes

(see the latest comparison here - https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/SelectEngine.html)

Choose Memcached if the following apply for you:
- You need the simplest model possible.
- You need to run large nodes with multiple cores or threads.
- You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases. 
- You need to cache objects, such as a database.

ElastiCache for Redis (Cluster mode enabled) version 3.2.4
If you need the functionality of Redis 2.8.x plus the following features, choose Redis 3.2.4 (clustered mode):
You need to partition your data across two to 90 node groups (clustered mode only).
You need geospatial indexing (clustered mode or non-clustered mode).
You don't need to support multiple databases.
Important
Redis (cluster mode enabled) has the following limitations:
No scale-up to larger node types
No changing the number of replicas in a node group (partition)
ElastiCache for Redis (non-clustered mode) 2.8x and 3.2.4 (Enhanced)
If the following apply for you, choose Redis 2.8.x or Redis 3.2.4 (non-clustered mode):
You need complex data types, such as strings, hashes, lists, sets, sorted sets, and bitmaps.
You need to sort or rank in-memory datasets.
You need persistence of your key store.
You need to replicate your data from the primary to one or more read replicas for read intensive applications.
You need automatic failover if your primary node fails.
You need publish and subscribe (pub/sub) capabilities—to inform clients about events on the server.
You need backup and restore capabilities.
You need to support multiple databases.

		
Aurora
	- MYSQL and Postgres SQL compatible relational database engine 
		- that combines the speed and availability of high-end commercial databases
		- with the simplicity and cost-effectiveness of open source databases
	- 5 times better performance than MYSQL 
	- at a price point one tenth that of a commercial database while delivering similar performance and availability
	- starts with 10GB, scales in 10GB increment to 64TB (Storage Autoscaling)
	- compute resources can scale upto v32CPUs and 244GB of Memory
	- 2 copies of your data in each availability zone, with minimum of 3 AZ. 6 copies of your data
	- note that Aurora is not currently supported in the region that does not have 3 AZs
	- it is designed to transparently handle 
		- the loss of upto 2 copies of data without affecting write availability
		- the loss of upto 3 copies of data without affecting read availability
	- Aurora storage is also self-healing
		- Data blocks and disks are continuously scanned for errors and repaired automatically
	- Comparsion
		Number of Replicas: Aurora: 15	MYSQL: 5
		Replication frequency: Aurora:millseconds MYSQL:seconds
		Automated failover: Aurora:Yes MYSQL:No
		Performance impact on primary: Aurora:Low MYSQL:High
	- Automated Backups are always enabled and no impact on performance
	- You can take snapshots with Aurora - again no impact on performance - share snapshots with other aws accounts
	- Database Migration simple steps
		- You can take a RDS instance and create a Aurora Read Replica
		- Once read replica is created, you can “Promote Read Replica” that will convert that into a working DB
		- You can also take a snapshot of the Aurora read replica and create another Aurora DB from that
	
	
		
DocumentDB
Neptune
