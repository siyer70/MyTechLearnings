brew install elasticsearch && brew info elasticsearch

==> /usr/local/Cellar/elasticsearch/6.8.3/bin/elasticsearch-keystore create
==> Caveats
Data:    /usr/local/var/lib/elasticsearch/
Logs:    /usr/local/var/log/elasticsearch/elasticsearch_chasrini.log
Plugins: /usr/local/var/elasticsearch/plugins/
Config:  /usr/local/etc/elasticsearch/

To have launchd start elasticsearch now and restart at login:
  brew services start elasticsearch
Or, if you don't want/need a background service you can just run:
  elasticsearch
==> Summary
ðŸº  /usr/local/Cellar/elasticsearch/6.8.3: 133 files, 103.2MB, built in 32 seconds


brew services start elasticsearch

WKMIN9419360:~ chasrini$ brew services start elasticsearch
==> Tapping homebrew/services
Cloning into '/usr/local/Homebrew/Library/Taps/homebrew/homebrew-services'...
remote: Enumerating objects: 14, done.
remote: Counting objects: 100% (14/14), done.
remote: Compressing objects: 100% (9/9), done.
remote: Total 14 (delta 0), reused 8 (delta 0), pack-reused 0
Unpacking objects: 100% (14/14), done.
Tapped 1 command (43 files, 59.5KB).
==> Successfully started `elasticsearch` (label: homebrew.mxcl.elasticsearch)


brew install logstash
==> Downloading https://artifacts.elastic.co/downloads/logstash/logstash-oss-7.3
######################################################################## 100.0%
==> Caveats
Configuration files are located in /usr/local/etc/logstash/

To have launchd start logstash now and restart at login:
  brew services start logstash
Or, if you don't want/need a background service you can just run:
  logstash
==> Summary
ðŸº  /usr/local/Cellar/logstash/7.3.2: 12,794 files, 287.7MB, built in 1 minute 15 seconds


brew services start logstash
==> Successfully started `logstash` (label: homebrew.mxcl.logstash)


brew install kibana

WKMIN9419360:~ chasrini$ brew install kibana
==> Downloading https://homebrew.bintray.com/bottles/kibana-6.8.3.mojave.bottle.
==> Downloading from https://akamai.bintray.com/f8/f8634e98d1d8ed79f691e7bf6faa4
######################################################################## 100.0%
==> Pouring kibana-6.8.3.mojave.bottle.tar.gz
==> Caveats
Config: /usr/local/etc/kibana/
If you wish to preserve your plugins upon upgrade, make a copy of
/usr/local/opt/kibana/plugins before upgrading, and copy it into the
new keg location after upgrading.

To have launchd start kibana now and restart at login:
  brew services start kibana
Or, if you don't want/need a background service you can just run:
  kibana
==> Summary
ðŸº  /usr/local/Cellar/kibana/6.8.3: 51,589 files, 254.0MB


brew services start kibana
==> Successfully started `kibana` (label: homebrew.mxcl.kibana)


brew services list
Name          Status  User     Plist
elasticsearch started chasrini /Users/chasrini/Library/LaunchAgents/homebrew.mxcl.elasticsearch.plist
kibana        started chasrini /Users/chasrini/Library/LaunchAgents/homebrew.mxcl.kibana.plist
logstash      started chasrini /Users/chasrini/Library/LaunchAgents/homebrew.mxcl.logstash.plist


logstash-plugin install logstash-filter-multiline


#delete all indexes
sudo curl -X DELETE 'http://localhost:9200/_all'


        multiline {
                patterns_dir => "/etc/logstash/patterns"
                pattern => "\[%{TOMCAT_DATESTAMP}"
                what => "previous"
        }
        if [type] == "leagueinfo_tomcat_logs" {
                mutate {
                        add_tag => [ "LEAGUEINFO_TOMCAT_LOG" ]
                }
                if "_grokparsefailure" in [tags] {
                        drop { }
                }
                date {
                        match => [ "timestamp", "UNIX_MS" ]
                        target => "@timestamp"
                }
        } else {
                drop { }
        }


filter {

	multiline {
		pattern => "^(%{TIMESTAMP_ISO8601})"
		negate => true
		what => "previous"
	}

	grok {
		# Do multiline matching with (?m) as the above mutliline filter may add newlines to the log messages.
		match => [ "message", "(?m)^%{TIMESTAMP_ISO8601:logtime}%{SPACE}%{LOGLEVEL:loglevel} %{SPACE}%{NUMBER:pid}%{SPACE}%{SYSLOG5424SD:threadname}%{SPACE}---%{SPACE}%{JAVACLASS:classname}%{SPACE}:%{SPACE}%{GREEDYDATA:logmessage}" ]
	}

}


        date {
                match => [ "timestamp", ISO8601 ]
                        target => "@timestamp"
        }





References

Java grok patterns
https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/java


Spring boot Logstash
https://blog.codecentric.de/en/2014/10/log-management-spring-boot-applications-logstash-elastichsearch-kibana/


Example didn't work
https://aggarwalarpit.wordpress.com/2015/12/03/configuring-elk-stack-to-analyse-apache-tomcat-logs/

Patterns
https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns






Elasticsearch Architecture
--------------------------
Source:
https://www.youtube.com/watch?v=YsYUgZu9-Y4

Jist of the content
- Index is split into shards
- every shard is a self-contained lucene index of its own (Lucene - a full text search library on which ElasticSearch is built on)
- each shard may be on a different node in a cluster
- every document is hashed into a particular shard
- you can scale the architecture by adding more machines to the cluster and more shards on these machines to split the load efficiently
- each node can have attributes that can be leveraged to specify that
	- it is running in which zone, which rack
	- it is running in a powerful machine because of a popular index
- cluster has so called "cluster state" that is replicated in every node. It has
	- Mappings (can be thought of as schema that specifies how a field has its text processed)
	- shard routing table - any node in the cluster knows how to route search requests
	- master node

Case study
An Index with 2 primary shards and 2 replicas

- write requests are sent to primary shards which are then replicated to replicas
	- so write requests are bottlenecked to number of primary shards that you have
	- Number of shards can be specified up front via the put request using REST / HTTP
		PUT /<index_name> 
		{
			"settings" : {
				"number_of_shards" : 3,
				"number_of_replicas" : 1
			}
		}
		total number of shards = 
			3 primary 
			3 replicas i.e. (1 replica for each primary)
			= 6 shards
- read requests can be sent to any of the primary / replicas subsequently
- If any of the primary node goes down, one of the other node can be elected as primary and can continue to serve requests
- Odd number nodes are preferred for resiliency / fault tolerance i.e. a minimum of 3

Limitations
- Number of primary shards cannot be changed later on
	- but you can add more replicas at any point in time to increase the read capacity
	- worst case you can reindex your data
- You cannot split one shard into two
- when querying on multiple shards, results are obtained from each shard and then collated
	- since shard can be across nodes, this would involve transfer over the network

- Trade Off - having two few shards or too many shards - both are problematic - you need to balance between the two

Lucene Index
- Comprises of
	- Segments (Mini Indexes)
		Segment Data Structure comprises of
			- An inverted index
			- Stored fields
			- Document Values and so on
- when searching within Lucene index, results are obtained from each segment and then colloated

Inverted index

Term		Frequency	Documents
secret		3		3
config		5		3, 5
partition	7		1, 2, 5
<------------------------>	<--------->
Sorted Dictionary		Postings

Process
a. lower case
b. remove punctuations
c. tokenize on white space

when you receive a query, you first lookup in sorted dictionary to find the candidate terms and then use the postings i.e. intersect / union the result based on the query

Search problems

Auto complete - find words starting with c 
Resolution: you can do a binary search here

Substring / wildcard search
- *suffix - you can reverse the string i.e. xiffus and then search

Geolocations (longtitude and lattitude)
- generate a geo hash combining both

guid range search
123 - {1 hundered, 12 tens, 3 ones} - 



